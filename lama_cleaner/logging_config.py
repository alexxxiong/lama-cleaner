"""
ç»Ÿä¸€æ—¥å¿—é…ç½®ç®¡ç†ç³»ç»Ÿ

æä¾›åˆ†å±‚é…ç½®ç®¡ç†ï¼Œæ”¯æŒå½©è‰²æ§åˆ¶å°è¾“å‡ºå’Œç»“æ„åŒ–æ—¥å¿—è®°å½•ã€‚
åŒ…å«é«˜çº§æ–‡ä»¶è½®è½¬ã€å‹ç¼©å’Œè‡ªåŠ¨æ¸…ç†åŠŸèƒ½ã€‚
"""

import sys
import os
import gzip
import shutil
from loguru import logger
from typing import Dict, Any, Optional, List
from pathlib import Path
import json
from datetime import datetime, timedelta
from dataclasses import dataclass, field
import threading
import time
import sqlite3
from typing import Union
import uuid


@dataclass
class FileRotationConfig:
    """æ–‡ä»¶è½®è½¬é…ç½®"""
    max_size: str = "100 MB"  # æœ€å¤§æ–‡ä»¶å¤§å°
    rotation_time: str = "1 day"  # è½®è½¬æ—¶é—´é—´éš”
    retention_days: int = 30  # ä¿ç•™å¤©æ•°
    retention_count: int = 10  # ä¿ç•™æ–‡ä»¶æ•°é‡
    compression: str = "gz"  # å‹ç¼©æ ¼å¼
    backup_count: int = 5  # å¤‡ä»½æ–‡ä»¶æ•°é‡
    
@dataclass 
class LoggingConfig:
    """å®Œæ•´çš„æ—¥å¿—é…ç½®"""
    level: str = "INFO"
    console_enabled: bool = True
    file_enabled: bool = True
    structured_logging: bool = True
    log_directory: str = "logs"
    filename_pattern: str = "lama_cleaner_{time:YYYY-MM-DD}.log"
    rotation: FileRotationConfig = field(default_factory=FileRotationConfig)


class LogFileManager:
    """æ—¥å¿—æ–‡ä»¶ç®¡ç†å™¨ - å¤„ç†è½®è½¬ã€å‹ç¼©å’Œæ¸…ç†"""
    
    def __init__(self, log_dir: Path, config: FileRotationConfig):
        self.log_dir = log_dir
        self.config = config
        self.cleanup_thread = None
        self.stop_cleanup = threading.Event()
        
    def start_cleanup_scheduler(self):
        """å¯åŠ¨å®šæœŸæ¸…ç†è°ƒåº¦å™¨"""
        if self.cleanup_thread is None or not self.cleanup_thread.is_alive():
            self.stop_cleanup.clear()
            self.cleanup_thread = threading.Thread(target=self._cleanup_worker, daemon=True)
            self.cleanup_thread.start()
            
    def stop_cleanup_scheduler(self):
        """åœæ­¢æ¸…ç†è°ƒåº¦å™¨"""
        if self.cleanup_thread and self.cleanup_thread.is_alive():
            self.stop_cleanup.set()
            self.cleanup_thread.join(timeout=5)
            
    def _cleanup_worker(self):
        """æ¸…ç†å·¥ä½œçº¿ç¨‹"""
        while not self.stop_cleanup.wait(3600):  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
            try:
                self.cleanup_old_logs()
                self.compress_old_logs()
            except Exception as e:
                logger.error(f"æ—¥å¿—æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                
    def cleanup_old_logs(self):
        """æ¸…ç†è¿‡æœŸçš„æ—¥å¿—æ–‡ä»¶"""
        if not self.log_dir.exists():
            return
            
        cutoff_date = datetime.now() - timedelta(days=self.config.retention_days)
        
        # è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        log_files = []
        for pattern in ["*.log", "*.log.gz", "*.log.bz2"]:
            log_files.extend(self.log_dir.glob(pattern))
            
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        log_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        
        deleted_count = 0
        
        # åˆ é™¤è¿‡æœŸæ–‡ä»¶
        for log_file in log_files:
            file_time = datetime.fromtimestamp(log_file.stat().st_mtime)
            if file_time < cutoff_date:
                try:
                    log_file.unlink()
                    deleted_count += 1
                    logger.debug(f"åˆ é™¤è¿‡æœŸæ—¥å¿—æ–‡ä»¶: {log_file.name}")
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ é™¤æ—¥å¿—æ–‡ä»¶ {log_file.name}: {e}")
                    
        # ä¿æŒæ–‡ä»¶æ•°é‡é™åˆ¶
        if len(log_files) > self.config.retention_count:
            files_to_delete = log_files[self.config.retention_count:]
            for log_file in files_to_delete:
                try:
                    if log_file.exists():
                        log_file.unlink()
                        deleted_count += 1
                        logger.debug(f"åˆ é™¤å¤šä½™æ—¥å¿—æ–‡ä»¶: {log_file.name}")
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ é™¤æ—¥å¿—æ–‡ä»¶ {log_file.name}: {e}")
                    
        if deleted_count > 0:
            logger.info(f"æ—¥å¿—æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ–‡ä»¶")
            
    def compress_old_logs(self):
        """å‹ç¼©æ—§çš„æ—¥å¿—æ–‡ä»¶"""
        if not self.log_dir.exists() or self.config.compression == "none":
            return
            
        # æŸ¥æ‰¾æœªå‹ç¼©çš„æ—¥å¿—æ–‡ä»¶ï¼ˆæ’é™¤å½“å‰æ—¥å¿—æ–‡ä»¶ï¼‰
        log_files = list(self.log_dir.glob("*.log"))
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        compressed_count = 0
        
        for log_file in log_files:
            # è·³è¿‡å½“å‰æ—¥æœŸçš„æ—¥å¿—æ–‡ä»¶
            if current_date in log_file.name:
                continue
                
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å‹ç¼©
            compressed_file = log_file.with_suffix(f".log.{self.config.compression}")
            if compressed_file.exists():
                continue
                
            try:
                if self.config.compression == "gz":
                    self._compress_gzip(log_file, compressed_file)
                elif self.config.compression == "bz2":
                    self._compress_bzip2(log_file, compressed_file)
                    
                # åˆ é™¤åŸæ–‡ä»¶
                log_file.unlink()
                compressed_count += 1
                logger.debug(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶: {log_file.name} -> {compressed_file.name}")
                
            except Exception as e:
                logger.warning(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file.name}: {e}")
                
        if compressed_count > 0:
            logger.info(f"æ—¥å¿—å‹ç¼©å®Œæˆï¼Œå‹ç¼©äº† {compressed_count} ä¸ªæ–‡ä»¶")
            
    def _compress_gzip(self, source: Path, target: Path):
        """ä½¿ç”¨gzipå‹ç¼©æ–‡ä»¶"""
        with open(source, 'rb') as f_in:
            with gzip.open(target, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
                
    def _compress_bzip2(self, source: Path, target: Path):
        """ä½¿ç”¨bzip2å‹ç¼©æ–‡ä»¶"""
        import bz2
        with open(source, 'rb') as f_in:
            with bz2.open(target, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
                
    def get_log_files_info(self) -> List[Dict[str, Any]]:
        """è·å–æ—¥å¿—æ–‡ä»¶ä¿¡æ¯"""
        if not self.log_dir.exists():
            return []
            
        files_info = []
        for log_file in self.log_dir.iterdir():
            if log_file.is_file() and any(log_file.name.endswith(ext) for ext in ['.log', '.log.gz', '.log.bz2']):
                stat = log_file.stat()
                files_info.append({
                    'name': log_file.name,
                    'size': stat.st_size,
                    'size_mb': round(stat.st_size / (1024 * 1024), 2),
                    'modified': datetime.fromtimestamp(stat.st_mtime),
                    'compressed': log_file.suffix in ['.gz', '.bz2']
                })
                
        return sorted(files_info, key=lambda x: x['modified'], reverse=True)


class StructuredLogStorage:
    """ç»“æ„åŒ–æ—¥å¿—å­˜å‚¨ç³»ç»Ÿ"""
    
    def __init__(self, db_path: str = "logs/structured_logs.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(exist_ok=True, parents=True)
        self._init_database()
        
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS log_entries (
                    id TEXT PRIMARY KEY,
                    timestamp DATETIME,
                    level TEXT,
                    module TEXT,
                    function TEXT,
                    line INTEGER,
                    message TEXT,
                    extra_data TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
            conn.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON log_entries(timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_level ON log_entries(level)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_module ON log_entries(module)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON log_entries(created_at)")
            
    def store_log_entry(self, record: Dict[str, Any]) -> str:
        """å­˜å‚¨æ—¥å¿—æ¡ç›®"""
        entry_id = str(uuid.uuid4())
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT INTO log_entries 
                    (id, timestamp, level, module, function, line, message, extra_data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    entry_id,
                    record.get('time'),
                    record.get('level'),
                    record.get('name'),
                    record.get('function'),
                    record.get('line'),
                    record.get('message'),
                    json.dumps(record.get('extra', {}), ensure_ascii=False)
                ))
                
        except Exception as e:
            logger.error(f"å­˜å‚¨æ—¥å¿—æ¡ç›®å¤±è´¥: {e}")
            
        return entry_id
        
    def query_logs(self, 
                   start_time: Optional[datetime] = None,
                   end_time: Optional[datetime] = None,
                   level: Optional[str] = None,
                   module: Optional[str] = None,
                   search_text: Optional[str] = None,
                   limit: int = 1000,
                   offset: int = 0) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æ—¥å¿—æ¡ç›®"""
        
        query = "SELECT * FROM log_entries WHERE 1=1"
        params = []
        
        if start_time:
            query += " AND timestamp >= ?"
            params.append(start_time.isoformat())
            
        if end_time:
            query += " AND timestamp <= ?"
            params.append(end_time.isoformat())
            
        if level:
            query += " AND level = ?"
            params.append(level)
            
        if module:
            query += " AND module LIKE ?"
            params.append(f"%{module}%")
            
        if search_text:
            query += " AND (message LIKE ? OR extra_data LIKE ?)"
            params.extend([f"%{search_text}%", f"%{search_text}%"])
            
        query += " ORDER BY timestamp DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.execute(query, params)
                
                results = []
                for row in cursor.fetchall():
                    entry = dict(row)
                    # è§£æJSONæ ¼å¼çš„extra_data
                    try:
                        entry['extra_data'] = json.loads(entry['extra_data']) if entry['extra_data'] else {}
                    except json.JSONDecodeError:
                        entry['extra_data'] = {}
                    results.append(entry)
                    
                return results
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ—¥å¿—å¤±è´¥: {e}")
            return []
            
    def get_log_statistics(self, 
                          start_time: Optional[datetime] = None,
                          end_time: Optional[datetime] = None) -> Dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        
        query = "SELECT level, COUNT(*) as count FROM log_entries WHERE 1=1"
        params = []
        
        if start_time:
            query += " AND timestamp >= ?"
            params.append(start_time.isoformat())
            
        if end_time:
            query += " AND timestamp <= ?"
            params.append(end_time.isoformat())
            
        query += " GROUP BY level"
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(query, params)
                level_stats = {row[0]: row[1] for row in cursor.fetchall()}
                
                # è·å–æ€»æ•°
                total_query = "SELECT COUNT(*) FROM log_entries WHERE 1=1"
                if params:
                    if start_time:
                        total_query += " AND timestamp >= ?"
                    if end_time:
                        total_query += " AND timestamp <= ?"
                        
                cursor = conn.execute(total_query, params)
                total_count = cursor.fetchone()[0]
                
                # è·å–æ¨¡å—ç»Ÿè®¡
                module_query = query.replace("level", "module")
                cursor = conn.execute(module_query, params)
                module_stats = {row[0]: row[1] for row in cursor.fetchall()}
                
                return {
                    "total_entries": total_count,
                    "level_distribution": level_stats,
                    "module_distribution": module_stats,
                    "query_time_range": {
                        "start": start_time.isoformat() if start_time else None,
                        "end": end_time.isoformat() if end_time else None
                    }
                }
                
        except Exception as e:
            logger.error(f"è·å–æ—¥å¿—ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
            
    def cleanup_old_entries(self, retention_days: int = 30) -> int:
        """æ¸…ç†æ—§çš„æ—¥å¿—æ¡ç›®"""
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(
                    "DELETE FROM log_entries WHERE timestamp < ?",
                    (cutoff_date.isoformat(),)
                )
                deleted_count = cursor.rowcount
                
                # ä¼˜åŒ–æ•°æ®åº“
                conn.execute("VACUUM")
                
                logger.info(f"æ¸…ç†äº† {deleted_count} æ¡æ—§æ—¥å¿—è®°å½•")
                return deleted_count
                
        except Exception as e:
            logger.error(f"æ¸…ç†æ—¥å¿—è®°å½•å¤±è´¥: {e}")
            return 0
            
    def export_to_json(self, output_path: str, 
                      start_time: Optional[datetime] = None,
                      end_time: Optional[datetime] = None) -> bool:
        """å¯¼å‡ºæ—¥å¿—ä¸ºJSONæ ¼å¼"""
        try:
            logs = self.query_logs(start_time=start_time, end_time=end_time, limit=10000)
            
            export_data = {
                "export_time": datetime.now().isoformat(),
                "total_entries": len(logs),
                "time_range": {
                    "start": start_time.isoformat() if start_time else None,
                    "end": end_time.isoformat() if end_time else None
                },
                "entries": logs
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"æ—¥å¿—å¯¼å‡ºå®Œæˆ: {output_path}, åŒ…å« {len(logs)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ—¥å¿—å¤±è´¥: {e}")
            return False
            
    def create_backup(self, backup_path: str) -> bool:
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        try:
            backup_file = Path(backup_path)
            backup_file.parent.mkdir(exist_ok=True, parents=True)
            
            shutil.copy2(self.db_path, backup_file)
            
            logger.info(f"æ•°æ®åº“å¤‡ä»½å®Œæˆ: {backup_file}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
            return False


class LogSearchEngine:
    """æ—¥å¿—æœç´¢å¼•æ“"""
    
    def __init__(self, storage: StructuredLogStorage):
        self.storage = storage
        
    def search(self, query: str, **filters) -> List[Dict[str, Any]]:
        """æœç´¢æ—¥å¿—æ¡ç›®"""
        return self.storage.query_logs(search_text=query, **filters)
        
    def search_by_pattern(self, pattern: str, **filters) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢"""
        import re
        
        # è·å–æ‰€æœ‰åŒ¹é…çš„æ—¥å¿—
        all_logs = self.storage.query_logs(**filters)
        
        try:
            regex = re.compile(pattern, re.IGNORECASE)
            filtered_logs = []
            
            for log in all_logs:
                if (regex.search(log['message']) or 
                    regex.search(json.dumps(log.get('extra_data', {})))):
                    filtered_logs.append(log)
                    
            return filtered_logs
            
        except re.error as e:
            logger.error(f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {e}")
            return []
            
    def get_error_summary(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–é”™è¯¯æ‘˜è¦"""
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=hours)
        
        error_logs = self.storage.query_logs(
            start_time=start_time,
            end_time=end_time,
            level="ERROR"
        )
        
        # æŒ‰é”™è¯¯æ¶ˆæ¯åˆ†ç»„
        error_groups = {}
        for log in error_logs:
            message = log['message']
            if message not in error_groups:
                error_groups[message] = {
                    'count': 0,
                    'first_occurrence': log['timestamp'],
                    'last_occurrence': log['timestamp'],
                    'modules': set()
                }
            
            error_groups[message]['count'] += 1
            error_groups[message]['modules'].add(log['module'])
            
            # æ›´æ–°æ—¶é—´èŒƒå›´
            if log['timestamp'] < error_groups[message]['first_occurrence']:
                error_groups[message]['first_occurrence'] = log['timestamp']
            if log['timestamp'] > error_groups[message]['last_occurrence']:
                error_groups[message]['last_occurrence'] = log['timestamp']
                
        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        for group in error_groups.values():
            group['modules'] = list(group['modules'])
            
        return {
            'time_range': f"æœ€è¿‘ {hours} å°æ—¶",
            'total_errors': len(error_logs),
            'unique_errors': len(error_groups),
            'error_groups': error_groups
        }


class LoggerManager:
    """ç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path
        self.is_setup = False
        self.config = LoggingConfig()
        self.file_manager: Optional[LogFileManager] = None
        self.structured_storage: Optional[StructuredLogStorage] = None
        self.search_engine: Optional[LogSearchEngine] = None
        
    def setup_logging(self, level: str = "INFO", enable_file_logging: bool = True, 
                     config: Optional[LoggingConfig] = None) -> None:
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        if self.is_setup:
            return
            
        # ä½¿ç”¨æä¾›çš„é…ç½®æˆ–é»˜è®¤é…ç½®
        if config:
            self.config = config
        else:
            self.config.level = level
            self.config.file_enabled = enable_file_logging
            
        # ç§»é™¤é»˜è®¤å¤„ç†å™¨
        logger.remove()
        
        # é…ç½®æ§åˆ¶å°è¾“å‡º
        if self.config.console_enabled:
            self._setup_console_handler(self.config.level)
        
        # é…ç½®æ–‡ä»¶è¾“å‡º
        if self.config.file_enabled:
            self._setup_file_handler(self.config.level)
            
        self.is_setup = True
        
    def load_config(self, config_path: str) -> LoggingConfig:
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            config_file = Path(config_path)
            if not config_file.exists():
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return LoggingConfig()
                
            with open(config_file, 'r', encoding='utf-8') as f:
                if config_path.endswith('.json'):
                    config_data = json.load(f)
                else:
                    # æ”¯æŒYAMLé…ç½®
                    try:
                        import yaml
                        config_data = yaml.safe_load(f)
                    except ImportError:
                        logger.error("éœ€è¦å®‰è£…PyYAMLæ¥æ”¯æŒYAMLé…ç½®æ–‡ä»¶")
                        return LoggingConfig()
                        
            # å°†é…ç½®æ•°æ®è½¬æ¢ä¸ºLoggingConfigå¯¹è±¡
            return self._dict_to_config(config_data)
            
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return LoggingConfig()
            
    def _dict_to_config(self, config_data: Dict[str, Any]) -> LoggingConfig:
        """å°†å­—å…¸è½¬æ¢ä¸ºé…ç½®å¯¹è±¡"""
        config = LoggingConfig()
        
        # åŸºæœ¬é…ç½®
        config.level = config_data.get('level', config.level)
        config.console_enabled = config_data.get('console_enabled', config.console_enabled)
        config.file_enabled = config_data.get('file_enabled', config.file_enabled)
        config.structured_logging = config_data.get('structured_logging', config.structured_logging)
        config.log_directory = config_data.get('log_directory', config.log_directory)
        config.filename_pattern = config_data.get('filename_pattern', config.filename_pattern)
        
        # è½®è½¬é…ç½®
        rotation_data = config_data.get('rotation', {})
        config.rotation.max_size = rotation_data.get('max_size', config.rotation.max_size)
        config.rotation.rotation_time = rotation_data.get('rotation_time', config.rotation.rotation_time)
        config.rotation.retention_days = rotation_data.get('retention_days', config.rotation.retention_days)
        config.rotation.retention_count = rotation_data.get('retention_count', config.rotation.retention_count)
        config.rotation.compression = rotation_data.get('compression', config.rotation.compression)
        config.rotation.backup_count = rotation_data.get('backup_count', config.rotation.backup_count)
        
        return config
        
    def _setup_console_handler(self, level: str) -> None:
        """é…ç½®å½©è‰²æ§åˆ¶å°è¾“å‡º"""
        
        def format_record(record):
            """è‡ªå®šä¹‰æ ¼å¼åŒ–å‡½æ•°ï¼Œæ·»åŠ emojiå’Œé¢œè‰²"""
            # æ ¹æ®æ—¥å¿—çº§åˆ«æ·»åŠ emojiå›¾æ ‡
            level_emojis = {
                "TRACE": "ğŸ”",
                "DEBUG": "ğŸ›", 
                "INFO": "â„¹ï¸",
                "SUCCESS": "âœ…",
                "WARNING": "âš ï¸",
                "ERROR": "âŒ",
                "CRITICAL": "ğŸ’¥"
            }
            
            # è·å–emojiå›¾æ ‡
            emoji = level_emojis.get(record["level"].name, "ğŸ“")
            
            # æ ¹æ®çº§åˆ«è®¾ç½®é¢œè‰²
            level_colors = {
                "TRACE": "dim",
                "DEBUG": "cyan",
                "INFO": "normal",
                "SUCCESS": "green",
                "WARNING": "yellow",
                "ERROR": "red",
                "CRITICAL": "red bold"
            }
            
            level_color = level_colors.get(record["level"].name, "normal")
            
            # æ ¼å¼åŒ–è¾“å‡º
            fmt = (
                "<green>{time:HH:mm:ss.SSS}</green> | "
                f"<{level_color}>{emoji} {{level: <8}}</{level_color}> | "
                "<cyan>{name: <20}</cyan> | "
                f"<{level_color}>{{message}}</{level_color}>"
            )
            
            return fmt
        
        logger.add(
            sys.stdout,
            format=format_record,
            level=level,
            colorize=True,
            backtrace=True,
            diagnose=True,
            enqueue=True  # çº¿ç¨‹å®‰å…¨
        )
        
    def _setup_file_handler(self, level: str) -> None:
        """é…ç½®é«˜çº§æ–‡ä»¶æ—¥å¿—è¾“å‡º"""
        log_dir = Path(self.config.log_directory)
        log_dir.mkdir(exist_ok=True, parents=True)
        
        # åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†å™¨
        self.file_manager = LogFileManager(log_dir, self.config.rotation)
        
        # æ ¹æ®é…ç½®é€‰æ‹©æ—¥å¿—æ ¼å¼
        if self.config.structured_logging:
            file_format = self._get_structured_format()
        else:
            file_format = (
                "{time:YYYY-MM-DD HH:mm:ss.SSS} | "
                "{level: <8} | "
                "{name}:{line} | "
                "{message}"
            )
        
        # é…ç½®ä¸»æ—¥å¿—æ–‡ä»¶
        log_file_path = log_dir / self.config.filename_pattern
        
        logger.add(
            log_file_path,
            format=file_format,
            level=level,
            rotation=self.config.rotation.max_size,
            retention=f"{self.config.rotation.retention_days} days",
            compression=self.config.rotation.compression if self.config.rotation.compression != "none" else None,
            backtrace=True,
            diagnose=True,
            enqueue=True,  # çº¿ç¨‹å®‰å…¨
            serialize=self.config.structured_logging  # JSONåºåˆ—åŒ–
        )
        
        # å¯åŠ¨æ–‡ä»¶ç®¡ç†å™¨
        self.file_manager.start_cleanup_scheduler()
        
        # å¦‚æœå¯ç”¨ç»“æ„åŒ–æ—¥å¿—ï¼Œåˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿ
        if self.config.structured_logging:
            self._setup_structured_storage(log_dir)
        
        logger.info(f"æ–‡ä»¶æ—¥å¿—å·²é…ç½®: {log_file_path}")
        logger.info(f"è½®è½¬ç­–ç•¥: å¤§å°={self.config.rotation.max_size}, ä¿ç•™={self.config.rotation.retention_days}å¤©")
        
    def _setup_structured_storage(self, log_dir: Path):
        """è®¾ç½®ç»“æ„åŒ–æ—¥å¿—å­˜å‚¨"""
        db_path = log_dir / "structured_logs.db"
        self.structured_storage = StructuredLogStorage(str(db_path))
        self.search_engine = LogSearchEngine(self.structured_storage)
        
        # æ·»åŠ è‡ªå®šä¹‰å¤„ç†å™¨æ¥å­˜å‚¨ç»“æ„åŒ–æ—¥å¿—
        def structured_handler(record):
            if self.structured_storage:
                self.structured_storage.store_log_entry(record)
                
        logger.add(
            structured_handler,
            level=self.config.level,
            format="{message}",  # ç®€å•æ ¼å¼ï¼Œå› ä¸ºæˆ‘ä»¬å­˜å‚¨å®Œæ•´çš„è®°å½•
            filter=lambda record: self.config.structured_logging
        )
        
        logger.info("ç»“æ„åŒ–æ—¥å¿—å­˜å‚¨å·²å¯ç”¨")
        
    def _get_structured_format(self) -> str:
        """è·å–ç»“æ„åŒ–æ—¥å¿—æ ¼å¼"""
        return (
            "{"
            '"timestamp": "{time:YYYY-MM-DD HH:mm:ss.SSS}", '
            '"level": "{level}", '
            '"module": "{name}", '
            '"line": {line}, '
            '"function": "{function}", '
            '"message": "{message}", '
            '"extra": {extra}'
            "}"
        )
        
    def get_logger(self, name: str):
        """è·å–æŒ‡å®šæ¨¡å—çš„æ—¥å¿—å™¨"""
        return logger.bind(name=name)
        
    def get_log_files_info(self) -> List[Dict[str, Any]]:
        """è·å–æ—¥å¿—æ–‡ä»¶ä¿¡æ¯"""
        if self.file_manager:
            return self.file_manager.get_log_files_info()
        return []
        
    def manual_cleanup(self) -> Dict[str, int]:
        """æ‰‹åŠ¨æ‰§è¡Œæ—¥å¿—æ¸…ç†"""
        if not self.file_manager:
            return {"deleted": 0, "compressed": 0}
            
        # è·å–æ¸…ç†å‰çš„æ–‡ä»¶æ•°é‡
        files_before = len(self.get_log_files_info())
        
        # æ‰§è¡Œæ¸…ç†
        self.file_manager.cleanup_old_logs()
        self.file_manager.compress_old_logs()
        
        # è·å–æ¸…ç†åçš„æ–‡ä»¶æ•°é‡
        files_after = len(self.get_log_files_info())
        
        return {
            "files_before": files_before,
            "files_after": files_after,
            "deleted": max(0, files_before - files_after)
        }
        
    def update_log_level(self, level: str) -> None:
        """åŠ¨æ€æ›´æ–°æ—¥å¿—çº§åˆ«"""
        try:
            # ç§»é™¤ç°æœ‰å¤„ç†å™¨
            logger.remove()
            
            # é‡æ–°è®¾ç½®å¤„ç†å™¨
            self.config.level = level
            self.is_setup = False
            self.setup_logging(level, self.config.file_enabled, self.config)
            
            logger.info(f"æ—¥å¿—çº§åˆ«å·²æ›´æ–°ä¸º: {level}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ—¥å¿—çº§åˆ«å¤±è´¥: {e}")
            
    def shutdown(self) -> None:
        """å…³é—­æ—¥å¿—ç®¡ç†å™¨"""
        if self.file_manager:
            self.file_manager.stop_cleanup_scheduler()
            
        logger.info("æ—¥å¿—ç®¡ç†å™¨å·²å…³é—­")
        
    def export_logs(self, output_path: str, date_range: Optional[tuple] = None) -> bool:
        """å¯¼å‡ºæ—¥å¿—æ–‡ä»¶"""
        try:
            output_file = Path(output_path)
            log_dir = Path(self.config.log_directory)
            
            if not log_dir.exists():
                logger.error("æ—¥å¿—ç›®å½•ä¸å­˜åœ¨")
                return False
                
            # æ”¶é›†è¦å¯¼å‡ºçš„æ—¥å¿—æ–‡ä»¶
            log_files = []
            for pattern in ["*.log", "*.log.gz", "*.log.bz2"]:
                log_files.extend(log_dir.glob(pattern))
                
            if date_range:
                start_date, end_date = date_range
                filtered_files = []
                for log_file in log_files:
                    file_time = datetime.fromtimestamp(log_file.stat().st_mtime)
                    if start_date <= file_time <= end_date:
                        filtered_files.append(log_file)
                log_files = filtered_files
                
            # åˆ›å»ºå‹ç¼©åŒ…
            import zipfile
            with zipfile.ZipFile(output_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for log_file in log_files:
                    zipf.write(log_file, log_file.name)
                    
            logger.info(f"æ—¥å¿—å¯¼å‡ºå®Œæˆ: {output_file}, åŒ…å« {len(log_files)} ä¸ªæ–‡ä»¶")
            return True
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ—¥å¿—å¤±è´¥: {e}")
            return False
            
    def search_logs(self, query: str, **filters) -> List[Dict[str, Any]]:
        """æœç´¢æ—¥å¿—"""
        if self.search_engine:
            return self.search_engine.search(query, **filters)
        return []
        
    def get_log_statistics(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        if self.structured_storage:
            end_time = datetime.now()
            start_time = end_time - timedelta(hours=hours)
            return self.structured_storage.get_log_statistics(start_time, end_time)
        return {}
        
    def get_error_summary(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–é”™è¯¯æ‘˜è¦"""
        if self.search_engine:
            return self.search_engine.get_error_summary(hours)
        return {}
        
    def export_structured_logs(self, output_path: str, 
                              start_time: Optional[datetime] = None,
                              end_time: Optional[datetime] = None) -> bool:
        """å¯¼å‡ºç»“æ„åŒ–æ—¥å¿—"""
        if self.structured_storage:
            return self.structured_storage.export_to_json(output_path, start_time, end_time)
        return False
        
    def backup_structured_logs(self, backup_path: str) -> bool:
        """å¤‡ä»½ç»“æ„åŒ–æ—¥å¿—æ•°æ®åº“"""
        if self.structured_storage:
            return self.structured_storage.create_backup(backup_path)
        return False
        
    def cleanup_structured_logs(self, retention_days: int = 30) -> int:
        """æ¸…ç†æ—§çš„ç»“æ„åŒ–æ—¥å¿—"""
        if self.structured_storage:
            return self.structured_storage.cleanup_old_entries(retention_days)
        return 0
        
    def show_startup_banner(self, version: str = "1.0.0", mode: str = "åˆ†å¸ƒå¼å¤„ç†", 
                            host: str = "localhost", port: int = 8080) -> None:
        """æ˜¾ç¤ºå¯åŠ¨æ¨ªå¹…"""
        import platform
        from datetime import datetime
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        py_version = platform.python_version()
        os_info = platform.system()
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ASCIIè‰ºæœ¯å­—
        ascii_art = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘    ğŸ¦™ LAMA CLEANER - Image Inpainting System                â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
        
        banner = f"""{ascii_art}

ğŸš€ ç³»ç»Ÿå¯åŠ¨ä¿¡æ¯
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ğŸ“¦ ç‰ˆæœ¬:      v{version}
   ğŸ¯ è¿è¡Œæ¨¡å¼:  {mode}
   ğŸŒ ç›‘å¬åœ°å€:  http://{host}:{port}
   ğŸ“ æ—¥å¿—ç›®å½•:  ./logs/
   ğŸ–¥ï¸  ç³»ç»Ÿç¯å¢ƒ:  {os_info} | Python {py_version}
   ğŸ• å¯åŠ¨æ—¶é—´:  {timestamp}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        print(banner)
        

# å…¨å±€æ—¥å¿—ç®¡ç†å™¨å®ä¾‹
_logger_manager = LoggerManager()

def setup_logging(level: str = "INFO", enable_file_logging: bool = True) -> None:
    """è®¾ç½®å…¨å±€æ—¥å¿—é…ç½®"""
    _logger_manager.setup_logging(level, enable_file_logging)

def get_logger(name: str):
    """è·å–æ¨¡å—æ—¥å¿—å™¨"""
    return _logger_manager.get_logger(name)

def show_startup_banner(version: str = "1.0.0", mode: str = "åˆ†å¸ƒå¼å¤„ç†", 
                       host: str = "localhost", port: int = 8080) -> None:
    """æ˜¾ç¤ºå¯åŠ¨æ¨ªå¹…"""
    _logger_manager.show_startup_banner(version, mode, host, port)

def log_success(message: str, name: str = "system") -> None:
    """è®°å½•æˆåŠŸæ¶ˆæ¯"""
    logger.bind(name=name).success(message)

def log_shutdown(name: str = "system") -> None:
    """æ˜¾ç¤ºä¼˜é›…å…³é—­æ¶ˆæ¯"""
    shutdown_msg = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    ğŸ›‘ ç³»ç»Ÿæ­£åœ¨ä¼˜é›…å…³é—­...                                    â•‘
â•‘    ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ Lama Cleaner!                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(shutdown_msg)
    logger.bind(name=name).info("ç³»ç»Ÿå·²å®‰å…¨å…³é—­")