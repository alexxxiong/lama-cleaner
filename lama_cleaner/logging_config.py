"""
ç»Ÿä¸€æ—¥å¿—é…ç½®ç®¡ç†ç³»ç»Ÿ

æä¾›åˆ†å±‚é…ç½®ç®¡ç†ï¼Œæ”¯æŒå½©è‰²æ§åˆ¶å°è¾“å‡ºå’Œç»“æ„åŒ–æ—¥å¿—è®°å½•ã€‚
åŒ…å«é«˜çº§æ–‡ä»¶è½®è½¬ã€å‹ç¼©å’Œè‡ªåŠ¨æ¸…ç†åŠŸèƒ½ã€‚
"""

import sys
import os
import gzip
import shutil
from loguru import logger
from typing import Dict, Any, Optional, List
from pathlib import Path
import json
from datetime import datetime, timedelta
from dataclasses import dataclass, field
import threading
import time
import sqlite3
from typing import Union
import uuid
from lama_cleaner.performance_monitor import performance_monitor, track_performance
from lama_cleaner.error_tracker import error_tracker, ErrorContext, ErrorCategory, ErrorSeverity
from lama_cleaner.privacy_protector import privacy_protector, PrivacyConfig, protect_message, protect_data


@dataclass
class FileRotationConfig:
    """æ–‡ä»¶è½®è½¬é…ç½®"""
    max_size: str = "100 MB"  # æœ€å¤§æ–‡ä»¶å¤§å°
    rotation_time: str = "1 day"  # è½®è½¬æ—¶é—´é—´éš”
    retention_days: int = 30  # ä¿ç•™å¤©æ•°
    retention_count: int = 10  # ä¿ç•™æ–‡ä»¶æ•°é‡
    compression: str = "gz"  # å‹ç¼©æ ¼å¼
    backup_count: int = 5  # å¤‡ä»½æ–‡ä»¶æ•°é‡
    
@dataclass 
class LoggingConfig:
    """å®Œæ•´çš„æ—¥å¿—é…ç½®"""
    level: str = "INFO"
    console_enabled: bool = True
    file_enabled: bool = True
    structured_logging: bool = True
    log_directory: str = "logs"
    filename_pattern: str = "lama_cleaner_{time:YYYY-MM-DD}.log"
    rotation: FileRotationConfig = field(default_factory=FileRotationConfig)


class LogFileManager:
    """æ—¥å¿—æ–‡ä»¶ç®¡ç†å™¨ - å¤„ç†è½®è½¬ã€å‹ç¼©å’Œæ¸…ç†"""
    
    def __init__(self, log_dir: Path, config: FileRotationConfig):
        self.log_dir = log_dir
        self.config = config
        self.cleanup_thread = None
        self.stop_cleanup = threading.Event()
        
    def start_cleanup_scheduler(self):
        """å¯åŠ¨å®šæœŸæ¸…ç†è°ƒåº¦å™¨"""
        if self.cleanup_thread is None or not self.cleanup_thread.is_alive():
            self.stop_cleanup.clear()
            self.cleanup_thread = threading.Thread(target=self._cleanup_worker, daemon=True)
            self.cleanup_thread.start()
            
    def stop_cleanup_scheduler(self):
        """åœæ­¢æ¸…ç†è°ƒåº¦å™¨"""
        if self.cleanup_thread and self.cleanup_thread.is_alive():
            self.stop_cleanup.set()
            self.cleanup_thread.join(timeout=5)
            
    def _cleanup_worker(self):
        """æ¸…ç†å·¥ä½œçº¿ç¨‹"""
        while not self.stop_cleanup.wait(3600):  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
            try:
                self.cleanup_old_logs()
                self.compress_old_logs()
            except Exception as e:
                logger.error(f"æ—¥å¿—æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                
    def cleanup_old_logs(self):
        """æ¸…ç†è¿‡æœŸçš„æ—¥å¿—æ–‡ä»¶"""
        if not self.log_dir.exists():
            return
            
        cutoff_date = datetime.now() - timedelta(days=self.config.retention_days)
        
        # è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        log_files = []
        for pattern in ["*.log", "*.log.gz", "*.log.bz2"]:
            log_files.extend(self.log_dir.glob(pattern))
            
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        log_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        
        deleted_count = 0
        
        # åˆ é™¤è¿‡æœŸæ–‡ä»¶
        for log_file in log_files:
            file_time = datetime.fromtimestamp(log_file.stat().st_mtime)
            if file_time < cutoff_date:
                try:
                    log_file.unlink()
                    deleted_count += 1
                    logger.debug(f"åˆ é™¤è¿‡æœŸæ—¥å¿—æ–‡ä»¶: {log_file.name}")
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ é™¤æ—¥å¿—æ–‡ä»¶ {log_file.name}: {e}")
                    
        # ä¿æŒæ–‡ä»¶æ•°é‡é™åˆ¶
        if len(log_files) > self.config.retention_count:
            files_to_delete = log_files[self.config.retention_count:]
            for log_file in files_to_delete:
                try:
                    if log_file.exists():
                        log_file.unlink()
                        deleted_count += 1
                        logger.debug(f"åˆ é™¤å¤šä½™æ—¥å¿—æ–‡ä»¶: {log_file.name}")
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ é™¤æ—¥å¿—æ–‡ä»¶ {log_file.name}: {e}")
                    
        if deleted_count > 0:
            logger.info(f"æ—¥å¿—æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ–‡ä»¶")
            
    def compress_old_logs(self):
        """å‹ç¼©æ—§çš„æ—¥å¿—æ–‡ä»¶"""
        if not self.log_dir.exists() or self.config.compression == "none":
            return
            
        # æŸ¥æ‰¾æœªå‹ç¼©çš„æ—¥å¿—æ–‡ä»¶ï¼ˆæ’é™¤å½“å‰æ—¥å¿—æ–‡ä»¶ï¼‰
        log_files = list(self.log_dir.glob("*.log"))
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        compressed_count = 0
        
        for log_file in log_files:
            # è·³è¿‡å½“å‰æ—¥æœŸçš„æ—¥å¿—æ–‡ä»¶
            if current_date in log_file.name:
                continue
                
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å‹ç¼©
            compressed_file = log_file.with_suffix(f".log.{self.config.compression}")
            if compressed_file.exists():
                continue
                
            try:
                if self.config.compression == "gz":
                    self._compress_gzip(log_file, compressed_file)
                elif self.config.compression == "bz2":
                    self._compress_bzip2(log_file, compressed_file)
                    
                # åˆ é™¤åŸæ–‡ä»¶
                log_file.unlink()
                compressed_count += 1
                logger.debug(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶: {log_file.name} -> {compressed_file.name}")
                
            except Exception as e:
                logger.warning(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file.name}: {e}")
                
        if compressed_count > 0:
            logger.info(f"æ—¥å¿—å‹ç¼©å®Œæˆï¼Œå‹ç¼©äº† {compressed_count} ä¸ªæ–‡ä»¶")
            
    def _compress_gzip(self, source: Path, target: Path):
        """ä½¿ç”¨gzipå‹ç¼©æ–‡ä»¶"""
        with open(source, 'rb') as f_in:
            with gzip.open(target, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
                
    def _compress_bzip2(self, source: Path, target: Path):
        """ä½¿ç”¨bzip2å‹ç¼©æ–‡ä»¶"""
        import bz2
        with open(source, 'rb') as f_in:
            with bz2.open(target, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
                
    def get_log_files_info(self) -> List[Dict[str, Any]]:
        """è·å–æ—¥å¿—æ–‡ä»¶ä¿¡æ¯"""
        if not self.log_dir.exists():
            return []
            
        files_info = []
        for log_file in self.log_dir.iterdir():
            if log_file.is_file() and any(log_file.name.endswith(ext) for ext in ['.log', '.log.gz', '.log.bz2']):
                stat = log_file.stat()
                files_info.append({
                    'name': log_file.name,
                    'size': stat.st_size,
                    'size_mb': round(stat.st_size / (1024 * 1024), 2),
                    'modified': datetime.fromtimestamp(stat.st_mtime),
                    'compressed': log_file.suffix in ['.gz', '.bz2']
                })
                
        return sorted(files_info, key=lambda x: x['modified'], reverse=True)


class StructuredLogStorage:
    """ç»“æ„åŒ–æ—¥å¿—å­˜å‚¨ç³»ç»Ÿ"""
    
    def __init__(self, db_path: str = "logs/structured_logs.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(exist_ok=True, parents=True)
        self._init_database()
        
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS log_entries (
                    id TEXT PRIMARY KEY,
                    timestamp DATETIME,
                    level TEXT,
                    module TEXT,
                    function TEXT,
                    line INTEGER,
                    message TEXT,
                    extra_data TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
            conn.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON log_entries(timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_level ON log_entries(level)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_module ON log_entries(module)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON log_entries(created_at)")
            
    def store_log_entry(self, record: Dict[str, Any]) -> str:
        """å­˜å‚¨æ—¥å¿—æ¡ç›®"""
        entry_id = str(uuid.uuid4())
        
        try:
            # å¤„ç†loguruçš„recordæ ¼å¼
            timestamp = record.get('time')
            if timestamp:
                timestamp = timestamp.isoformat() if hasattr(timestamp, 'isoformat') else str(timestamp)
            
            level = record.get('level')
            if level and hasattr(level, 'name'):
                level = level.name
            elif level:
                level = str(level)
                
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT INTO log_entries 
                    (id, timestamp, level, module, function, line, message, extra_data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    entry_id,
                    timestamp,
                    level,
                    record.get('name'),
                    record.get('function'),
                    record.get('line'),
                    record.get('message'),
                    json.dumps(record.get('extra', {}), ensure_ascii=False)
                ))
                
        except Exception as e:
            # é¿å…åœ¨æ—¥å¿—å¤„ç†å™¨ä¸­å†æ¬¡è®°å½•é”™è¯¯ï¼Œé˜²æ­¢é€’å½’
            pass
            
        return entry_id
        
    def query_logs(self, 
                   start_time: Optional[datetime] = None,
                   end_time: Optional[datetime] = None,
                   level: Optional[str] = None,
                   module: Optional[str] = None,
                   search_text: Optional[str] = None,
                   limit: int = 1000,
                   offset: int = 0) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æ—¥å¿—æ¡ç›®"""
        
        query = "SELECT * FROM log_entries WHERE 1=1"
        params = []
        
        if start_time:
            query += " AND timestamp >= ?"
            params.append(start_time.isoformat())
            
        if end_time:
            query += " AND timestamp <= ?"
            params.append(end_time.isoformat())
            
        if level:
            query += " AND level = ?"
            params.append(level)
            
        if module:
            query += " AND module LIKE ?"
            params.append(f"%{module}%")
            
        if search_text:
            query += " AND (message LIKE ? OR extra_data LIKE ?)"
            params.extend([f"%{search_text}%", f"%{search_text}%"])
            
        query += " ORDER BY timestamp DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.execute(query, params)
                
                results = []
                for row in cursor.fetchall():
                    entry = dict(row)
                    # è§£æJSONæ ¼å¼çš„extra_data
                    try:
                        entry['extra_data'] = json.loads(entry['extra_data']) if entry['extra_data'] else {}
                    except json.JSONDecodeError:
                        entry['extra_data'] = {}
                    results.append(entry)
                    
                return results
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ—¥å¿—å¤±è´¥: {e}")
            return []
            
    def get_log_statistics(self, 
                          start_time: Optional[datetime] = None,
                          end_time: Optional[datetime] = None) -> Dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        
        query = "SELECT level, COUNT(*) as count FROM log_entries WHERE 1=1"
        params = []
        
        if start_time:
            query += " AND timestamp >= ?"
            params.append(start_time.isoformat())
            
        if end_time:
            query += " AND timestamp <= ?"
            params.append(end_time.isoformat())
            
        query += " GROUP BY level"
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(query, params)
                level_stats = {row[0]: row[1] for row in cursor.fetchall()}
                
                # è·å–æ€»æ•°
                total_query = "SELECT COUNT(*) FROM log_entries WHERE 1=1"
                if params:
                    if start_time:
                        total_query += " AND timestamp >= ?"
                    if end_time:
                        total_query += " AND timestamp <= ?"
                        
                cursor = conn.execute(total_query, params)
                total_count = cursor.fetchone()[0]
                
                # è·å–æ¨¡å—ç»Ÿè®¡
                module_query = query.replace("level", "module")
                cursor = conn.execute(module_query, params)
                module_stats = {row[0]: row[1] for row in cursor.fetchall()}
                
                return {
                    "total_entries": total_count,
                    "level_distribution": level_stats,
                    "module_distribution": module_stats,
                    "query_time_range": {
                        "start": start_time.isoformat() if start_time else None,
                        "end": end_time.isoformat() if end_time else None
                    }
                }
                
        except Exception as e:
            logger.error(f"è·å–æ—¥å¿—ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
            
    def cleanup_old_entries(self, retention_days: int = 30) -> int:
        """æ¸…ç†æ—§çš„æ—¥å¿—æ¡ç›®"""
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.execute(
                "DELETE FROM log_entries WHERE timestamp < ?",
                (cutoff_date.isoformat(),)
            )
            deleted_count = cursor.rowcount
            conn.commit()
            conn.close()
            
            # åœ¨å•ç‹¬çš„è¿æ¥ä¸­æ‰§è¡ŒVACUUM
            conn = sqlite3.connect(self.db_path)
            conn.execute("VACUUM")
            conn.close()
            
            if deleted_count > 0:
                logger.info(f"æ¸…ç†äº† {deleted_count} æ¡æ—§æ—¥å¿—è®°å½•")
            return deleted_count
                
        except Exception as e:
            logger.error(f"æ¸…ç†æ—¥å¿—è®°å½•å¤±è´¥: {e}")
            return 0
            
    def export_to_json(self, output_path: str, 
                      start_time: Optional[datetime] = None,
                      end_time: Optional[datetime] = None) -> bool:
        """å¯¼å‡ºæ—¥å¿—ä¸ºJSONæ ¼å¼"""
        try:
            logs = self.query_logs(start_time=start_time, end_time=end_time, limit=10000)
            
            export_data = {
                "export_time": datetime.now().isoformat(),
                "total_entries": len(logs),
                "time_range": {
                    "start": start_time.isoformat() if start_time else None,
                    "end": end_time.isoformat() if end_time else None
                },
                "entries": logs
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"æ—¥å¿—å¯¼å‡ºå®Œæˆ: {output_path}, åŒ…å« {len(logs)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ—¥å¿—å¤±è´¥: {e}")
            return False
            
    def create_backup(self, backup_path: str) -> bool:
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        try:
            backup_file = Path(backup_path)
            backup_file.parent.mkdir(exist_ok=True, parents=True)
            
            shutil.copy2(self.db_path, backup_file)
            
            logger.info(f"æ•°æ®åº“å¤‡ä»½å®Œæˆ: {backup_file}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
            return False


class LogSearchEngine:
    """æ—¥å¿—æœç´¢å¼•æ“"""
    
    def __init__(self, storage: StructuredLogStorage):
        self.storage = storage
        
    def search(self, query: str, **filters) -> List[Dict[str, Any]]:
        """æœç´¢æ—¥å¿—æ¡ç›®"""
        return self.storage.query_logs(search_text=query, **filters)
        
    def search_by_pattern(self, pattern: str, **filters) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢"""
        import re
        
        # è·å–æ‰€æœ‰åŒ¹é…çš„æ—¥å¿—
        all_logs = self.storage.query_logs(**filters)
        
        try:
            regex = re.compile(pattern, re.IGNORECASE)
            filtered_logs = []
            
            for log in all_logs:
                if (regex.search(log['message']) or 
                    regex.search(json.dumps(log.get('extra_data', {})))):
                    filtered_logs.append(log)
                    
            return filtered_logs
            
        except re.error as e:
            logger.error(f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {e}")
            return []
            
    def get_error_summary(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–é”™è¯¯æ‘˜è¦"""
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=hours)
        
        error_logs = self.storage.query_logs(
            start_time=start_time,
            end_time=end_time,
            level="ERROR"
        )
        
        # æŒ‰é”™è¯¯æ¶ˆæ¯åˆ†ç»„
        error_groups = {}
        for log in error_logs:
            message = log['message']
            if message not in error_groups:
                error_groups[message] = {
                    'count': 0,
                    'first_occurrence': log['timestamp'],
                    'last_occurrence': log['timestamp'],
                    'modules': set()
                }
            
            error_groups[message]['count'] += 1
            error_groups[message]['modules'].add(log['module'])
            
            # æ›´æ–°æ—¶é—´èŒƒå›´
            if log['timestamp'] < error_groups[message]['first_occurrence']:
                error_groups[message]['first_occurrence'] = log['timestamp']
            if log['timestamp'] > error_groups[message]['last_occurrence']:
                error_groups[message]['last_occurrence'] = log['timestamp']
                
        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        for group in error_groups.values():
            group['modules'] = list(group['modules'])
            
        return {
            'time_range': f"æœ€è¿‘ {hours} å°æ—¶",
            'total_errors': len(error_logs),
            'unique_errors': len(error_groups),
            'error_groups': error_groups
        }


class LoggerManager:
    """ç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path
        self.is_setup = False
        self.config = LoggingConfig()
        self.file_manager: Optional[LogFileManager] = None
        self.structured_storage: Optional[StructuredLogStorage] = None
        self.search_engine: Optional[LogSearchEngine] = None
        self.privacy_enabled = True  # é»˜è®¤å¯ç”¨éšç§ä¿æŠ¤
        self.privacy_config = PrivacyConfig()  # éšç§ä¿æŠ¤é…ç½®
        
    def setup_logging(self, level: str = "INFO", enable_file_logging: bool = True, 
                     config: Optional[LoggingConfig] = None) -> None:
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        if self.is_setup:
            return
            
        # ä½¿ç”¨æä¾›çš„é…ç½®æˆ–é»˜è®¤é…ç½®
        if config:
            self.config = config
        else:
            self.config.level = level
            self.config.file_enabled = enable_file_logging
            
        # ç§»é™¤é»˜è®¤å¤„ç†å™¨
        logger.remove()
        
        # é…ç½®æ§åˆ¶å°è¾“å‡º
        if self.config.console_enabled:
            self._setup_console_handler(self.config.level)
        
        # é…ç½®æ–‡ä»¶è¾“å‡º
        if self.config.file_enabled:
            self._setup_file_handler(self.config.level)
            
        self.is_setup = True
        
    def load_config(self, config_path: str) -> LoggingConfig:
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            config_file = Path(config_path)
            if not config_file.exists():
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return LoggingConfig()
                
            with open(config_file, 'r', encoding='utf-8') as f:
                if config_path.endswith('.json'):
                    config_data = json.load(f)
                else:
                    # æ”¯æŒYAMLé…ç½®
                    try:
                        import yaml
                        config_data = yaml.safe_load(f)
                    except ImportError:
                        logger.error("éœ€è¦å®‰è£…PyYAMLæ¥æ”¯æŒYAMLé…ç½®æ–‡ä»¶")
                        return LoggingConfig()
                        
            # å°†é…ç½®æ•°æ®è½¬æ¢ä¸ºLoggingConfigå¯¹è±¡
            return self._dict_to_config(config_data)
            
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return LoggingConfig()
            
    def _dict_to_config(self, config_data: Dict[str, Any]) -> LoggingConfig:
        """å°†å­—å…¸è½¬æ¢ä¸ºé…ç½®å¯¹è±¡"""
        config = LoggingConfig()
        
        # åŸºæœ¬é…ç½®
        config.level = config_data.get('level', config.level)
        config.console_enabled = config_data.get('console_enabled', config.console_enabled)
        config.file_enabled = config_data.get('file_enabled', config.file_enabled)
        config.structured_logging = config_data.get('structured_logging', config.structured_logging)
        config.log_directory = config_data.get('log_directory', config.log_directory)
        config.filename_pattern = config_data.get('filename_pattern', config.filename_pattern)
        
        # è½®è½¬é…ç½®
        rotation_data = config_data.get('rotation', {})
        config.rotation.max_size = rotation_data.get('max_size', config.rotation.max_size)
        config.rotation.rotation_time = rotation_data.get('rotation_time', config.rotation.rotation_time)
        config.rotation.retention_days = rotation_data.get('retention_days', config.rotation.retention_days)
        config.rotation.retention_count = rotation_data.get('retention_count', config.rotation.retention_count)
        config.rotation.compression = rotation_data.get('compression', config.rotation.compression)
        config.rotation.backup_count = rotation_data.get('backup_count', config.rotation.backup_count)
        
        return config
        
    def _setup_console_handler(self, level: str) -> None:
        """é…ç½®å½©è‰²æ§åˆ¶å°è¾“å‡º"""
        
        def format_record(record):
            """è‡ªå®šä¹‰æ ¼å¼åŒ–å‡½æ•°ï¼Œæ·»åŠ emojiå’Œé¢œè‰²"""
            # æ ¹æ®æ—¥å¿—çº§åˆ«æ·»åŠ emojiå›¾æ ‡
            level_emojis = {
                "TRACE": "ğŸ”",
                "DEBUG": "ğŸ›", 
                "INFO": "â„¹ï¸",
                "SUCCESS": "âœ…",
                "WARNING": "âš ï¸",
                "ERROR": "âŒ",
                "CRITICAL": "ğŸ’¥"
            }
            
            # è·å–emojiå›¾æ ‡
            emoji = level_emojis.get(record["level"].name, "ğŸ“")
            
            # æ ¹æ®çº§åˆ«è®¾ç½®é¢œè‰²
            level_colors = {
                "TRACE": "dim",
                "DEBUG": "cyan",
                "INFO": "normal",
                "SUCCESS": "green",
                "WARNING": "yellow",
                "ERROR": "red",
                "CRITICAL": "red bold"
            }
            
            level_color = level_colors.get(record["level"].name, "normal")
            
            # æ ¼å¼åŒ–è¾“å‡º
            fmt = (
                "<green>{time:HH:mm:ss.SSS}</green> | "
                f"<{level_color}>{emoji} {{level: <8}}</{level_color}> | "
                "<cyan>{name: <20}</cyan> | "
                f"<{level_color}>{{message}}</{level_color}>"
            )
            
            return fmt
        
        # æ·»åŠ å¸¦éšç§ä¿æŠ¤çš„è¿‡æ»¤å™¨
        def privacy_filter(record):
            if self.privacy_enabled:
                # ä¿æŠ¤æ¶ˆæ¯ä¸­çš„æ•æ„Ÿä¿¡æ¯
                record["message"] = protect_message(record["message"])
                # ä¿æŠ¤é¢å¤–æ•°æ®
                if "extra" in record and isinstance(record["extra"], dict):
                    record["extra"] = protect_data(record["extra"])
            return True
            
        logger.add(
            sys.stdout,
            format=format_record,
            level=level,
            colorize=True,
            backtrace=True,
            filter=privacy_filter,
            diagnose=True,
            enqueue=True  # çº¿ç¨‹å®‰å…¨
        )
        
    def _setup_file_handler(self, level: str) -> None:
        """é…ç½®é«˜çº§æ–‡ä»¶æ—¥å¿—è¾“å‡º"""
        log_dir = Path(self.config.log_directory)
        log_dir.mkdir(exist_ok=True, parents=True)
        
        # åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†å™¨
        self.file_manager = LogFileManager(log_dir, self.config.rotation)
        
        # æ ¹æ®é…ç½®é€‰æ‹©æ—¥å¿—æ ¼å¼
        if self.config.structured_logging:
            file_format = self._get_structured_format()
        else:
            file_format = (
                "{time:YYYY-MM-DD HH:mm:ss.SSS} | "
                "{level: <8} | "
                "{name}:{line} | "
                "{message}"
            )
        
        # é…ç½®ä¸»æ—¥å¿—æ–‡ä»¶
        log_file_path = log_dir / self.config.filename_pattern
        
        # æ–‡ä»¶æ—¥å¿—çš„éšç§ä¿æŠ¤è¿‡æ»¤å™¨
        def file_privacy_filter(record):
            if self.privacy_enabled:
                record["message"] = protect_message(record["message"])
                if "extra" in record and isinstance(record["extra"], dict):
                    record["extra"] = protect_data(record["extra"])
            return True
            
        logger.add(
            log_file_path,
            format=file_format,
            level=level,
            rotation=self.config.rotation.max_size,
            retention=f"{self.config.rotation.retention_days} days",
            compression=self.config.rotation.compression if self.config.rotation.compression != "none" else None,
            backtrace=True,
            diagnose=True,
            enqueue=True,  # çº¿ç¨‹å®‰å…¨
            serialize=self.config.structured_logging,  # JSONåºåˆ—åŒ–
            filter=file_privacy_filter
        )
        
        # å¯åŠ¨æ–‡ä»¶ç®¡ç†å™¨
        self.file_manager.start_cleanup_scheduler()
        
        # å¦‚æœå¯ç”¨ç»“æ„åŒ–æ—¥å¿—ï¼Œåˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿ
        if self.config.structured_logging:
            self._setup_structured_storage(log_dir)
        
        logger.info(f"æ–‡ä»¶æ—¥å¿—å·²é…ç½®: {log_file_path}")
        logger.info(f"è½®è½¬ç­–ç•¥: å¤§å°={self.config.rotation.max_size}, ä¿ç•™={self.config.rotation.retention_days}å¤©")
        
    def _setup_structured_storage(self, log_dir: Path):
        """è®¾ç½®ç»“æ„åŒ–æ—¥å¿—å­˜å‚¨"""
        db_path = log_dir / "structured_logs.db"
        self.structured_storage = StructuredLogStorage(str(db_path))
        self.search_engine = LogSearchEngine(self.structured_storage)
        
        # æ·»åŠ è‡ªå®šä¹‰å¤„ç†å™¨æ¥å­˜å‚¨ç»“æ„åŒ–æ—¥å¿—
        def structured_handler(message):
            if self.structured_storage:
                # message.record åŒ…å«å®Œæ•´çš„æ—¥å¿—è®°å½•ä¿¡æ¯
                record_dict = message.record
                self.structured_storage.store_log_entry(record_dict)
                
        logger.add(
            structured_handler,
            level=self.config.level,
            format="{message}",  # ç®€å•æ ¼å¼ï¼Œå› ä¸ºæˆ‘ä»¬å­˜å‚¨å®Œæ•´çš„è®°å½•
            filter=lambda record: self.config.structured_logging
        )
        
        logger.info("ç»“æ„åŒ–æ—¥å¿—å­˜å‚¨å·²å¯ç”¨")
        
    def _get_structured_format(self) -> str:
        """è·å–ç»“æ„åŒ–æ—¥å¿—æ ¼å¼"""
        return "{message}"  # ä½¿ç”¨serialize=Trueæ—¶ï¼Œloguruä¼šè‡ªåŠ¨å¤„ç†JSONæ ¼å¼
        
    def get_logger(self, name: str):
        """è·å–æŒ‡å®šæ¨¡å—çš„æ—¥å¿—å™¨"""
        return logger.bind(name=name)
        
    def get_log_files_info(self) -> List[Dict[str, Any]]:
        """è·å–æ—¥å¿—æ–‡ä»¶ä¿¡æ¯"""
        if self.file_manager:
            return self.file_manager.get_log_files_info()
        return []
        
    def manual_cleanup(self) -> Dict[str, int]:
        """æ‰‹åŠ¨æ‰§è¡Œæ—¥å¿—æ¸…ç†"""
        if not self.file_manager:
            return {"deleted": 0, "compressed": 0}
            
        # è·å–æ¸…ç†å‰çš„æ–‡ä»¶æ•°é‡
        files_before = len(self.get_log_files_info())
        
        # æ‰§è¡Œæ¸…ç†
        self.file_manager.cleanup_old_logs()
        self.file_manager.compress_old_logs()
        
        # è·å–æ¸…ç†åçš„æ–‡ä»¶æ•°é‡
        files_after = len(self.get_log_files_info())
        
        return {
            "files_before": files_before,
            "files_after": files_after,
            "deleted": max(0, files_before - files_after)
        }
        
    def update_log_level(self, level: str) -> None:
        """åŠ¨æ€æ›´æ–°æ—¥å¿—çº§åˆ«"""
        try:
            # ç§»é™¤ç°æœ‰å¤„ç†å™¨
            logger.remove()
            
            # é‡æ–°è®¾ç½®å¤„ç†å™¨
            self.config.level = level
            self.is_setup = False
            self.setup_logging(level, self.config.file_enabled, self.config)
            
            logger.info(f"æ—¥å¿—çº§åˆ«å·²æ›´æ–°ä¸º: {level}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ—¥å¿—çº§åˆ«å¤±è´¥: {e}")
            
    def shutdown(self) -> None:
        """å…³é—­æ—¥å¿—ç®¡ç†å™¨"""
        if self.file_manager:
            self.file_manager.stop_cleanup_scheduler()
            
        # åœæ­¢æ€§èƒ½ç›‘æ§
        performance_monitor.stop()
        
        logger.info("æ—¥å¿—ç®¡ç†å™¨å·²å…³é—­")
        
    def export_logs(self, output_path: str, date_range: Optional[tuple] = None) -> bool:
        """å¯¼å‡ºæ—¥å¿—æ–‡ä»¶"""
        try:
            output_file = Path(output_path)
            log_dir = Path(self.config.log_directory)
            
            if not log_dir.exists():
                logger.error("æ—¥å¿—ç›®å½•ä¸å­˜åœ¨")
                return False
                
            # æ”¶é›†è¦å¯¼å‡ºçš„æ—¥å¿—æ–‡ä»¶
            log_files = []
            for pattern in ["*.log", "*.log.gz", "*.log.bz2"]:
                log_files.extend(log_dir.glob(pattern))
                
            if date_range:
                start_date, end_date = date_range
                filtered_files = []
                for log_file in log_files:
                    file_time = datetime.fromtimestamp(log_file.stat().st_mtime)
                    if start_date <= file_time <= end_date:
                        filtered_files.append(log_file)
                log_files = filtered_files
                
            # åˆ›å»ºå‹ç¼©åŒ…
            import zipfile
            with zipfile.ZipFile(output_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for log_file in log_files:
                    zipf.write(log_file, log_file.name)
                    
            logger.info(f"æ—¥å¿—å¯¼å‡ºå®Œæˆ: {output_file}, åŒ…å« {len(log_files)} ä¸ªæ–‡ä»¶")
            return True
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ—¥å¿—å¤±è´¥: {e}")
            return False
            
    def search_logs(self, query: str, **filters) -> List[Dict[str, Any]]:
        """æœç´¢æ—¥å¿—"""
        if self.search_engine:
            return self.search_engine.search(query, **filters)
        return []
        
    def get_log_statistics(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        if self.structured_storage:
            end_time = datetime.now()
            start_time = end_time - timedelta(hours=hours)
            return self.structured_storage.get_log_statistics(start_time, end_time)
        return {}
        
    def get_error_summary(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–é”™è¯¯æ‘˜è¦"""
        if self.search_engine:
            return self.search_engine.get_error_summary(hours)
        return {}
        
    def start_performance_monitoring(self, interval: int = 5) -> None:
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        try:
            performance_monitor.resource_monitor.interval = interval
            performance_monitor.start()
            logger.info(f"æ€§èƒ½ç›‘æ§å·²å¯åŠ¨ï¼Œç›‘æ§é—´éš”: {interval}ç§’")
        except Exception as e:
            logger.error(f"å¯åŠ¨æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
            
    def stop_performance_monitoring(self) -> None:
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        try:
            performance_monitor.stop()
            logger.info("æ€§èƒ½ç›‘æ§å·²åœæ­¢")
        except Exception as e:
            logger.error(f"åœæ­¢æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
            
    def get_performance_status(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç›‘æ§çŠ¶æ€"""
        return performance_monitor.get_status()
        
    def get_current_metrics(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç³»ç»Ÿèµ„æºæŒ‡æ ‡"""
        return performance_monitor.get_metrics()
        
    def log_performance_report(self) -> None:
        """è®°å½•æ€§èƒ½æŠ¥å‘Šåˆ°æ—¥å¿—"""
        try:
            report = performance_monitor.performance_tracker.get_performance_report()
            
            logger.info("=" * 50)
            logger.info("ğŸ“Š æ€§èƒ½ç›‘æ§æŠ¥å‘Š")
            logger.info("=" * 50)
            
            # èµ„æºä½¿ç”¨æ‘˜è¦
            metrics = self.get_current_metrics()
            logger.info("ğŸ–¥ï¸ ç³»ç»Ÿèµ„æºä½¿ç”¨:")
            logger.info(f"  â€¢ CPU: {metrics['cpu']['percent']:.1f}% ({metrics['cpu']['count']}æ ¸å¿ƒ)")
            logger.info(f"  â€¢ å†…å­˜: {metrics['memory']['percent']:.1f}% "
                       f"({metrics['memory']['used']:.1f}GB / {metrics['memory']['total']:.1f}GB)")
            logger.info(f"  â€¢ ç£ç›˜: {metrics['disk']['percent']:.1f}% "
                       f"({metrics['disk']['used']:.1f}GB / {metrics['disk']['total']:.1f}GB)")
            
            # GPUä¿¡æ¯
            if 'gpu' in metrics and metrics['gpu']:
                logger.info("ğŸ® GPUä½¿ç”¨:")
                for gpu in metrics['gpu']:
                    logger.info(f"  â€¢ GPU {gpu['id']}: {gpu['name']}")
                    logger.info(f"    è´Ÿè½½: {gpu['load']:.1f}%, "
                               f"å†…å­˜: {gpu['memory_percent']:.1f}% "
                               f"({gpu['memory_used']}MB / {gpu['memory_total']}MB)")
            
            # æ“ä½œæ€§èƒ½ç»Ÿè®¡
            if report['operations']:
                logger.info("\nâš¡ æ“ä½œæ€§èƒ½ç»Ÿè®¡:")
                for name, stats in report['operations'].items():
                    logger.info(f"  â€¢ {name}:")
                    logger.info(f"    - æ‰§è¡Œæ¬¡æ•°: {stats['count']}")
                    logger.info(f"    - å¹³å‡è€—æ—¶: {stats['avg_time']:.3f}ç§’")
                    logger.info(f"    - æœ€å°/æœ€å¤§: {stats['min_time']:.3f}s / {stats['max_time']:.3f}s")
                    if stats['error_rate'] > 0:
                        logger.warning(f"    - é”™è¯¯ç‡: {stats['error_rate']:.1f}%")
            
            # æ€§èƒ½ç“¶é¢ˆ
            if report['bottlenecks']:
                logger.warning("\nâš ï¸ æ£€æµ‹åˆ°æ€§èƒ½ç“¶é¢ˆ:")
                for bottleneck in report['bottlenecks']:
                    severity_emoji = "ğŸ”´" if bottleneck['severity'] == 'high' else "ğŸŸ¡"
                    logger.warning(f"  {severity_emoji} {bottleneck['operation']}: "
                                 f"å¹³å‡è€—æ—¶ {bottleneck['avg_time']:.2f}ç§’")
            
            # å»ºè®®
            if report['recommendations']:
                logger.info("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for rec in report['recommendations']:
                    logger.info(f"  â€¢ {rec}")
            
            logger.info("=" * 50)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
    
    def export_structured_logs(self, output_path: str, 
                              start_time: Optional[datetime] = None,
                              end_time: Optional[datetime] = None) -> bool:
        """å¯¼å‡ºç»“æ„åŒ–æ—¥å¿—"""
        if self.structured_storage:
            return self.structured_storage.export_to_json(output_path, start_time, end_time)
        return False
        
    def backup_structured_logs(self, backup_path: str) -> bool:
        """å¤‡ä»½ç»“æ„åŒ–æ—¥å¿—æ•°æ®åº“"""
        if self.structured_storage:
            return self.structured_storage.create_backup(backup_path)
        return False
        
    def cleanup_structured_logs(self, retention_days: int = 30) -> int:
        """æ¸…ç†æ—§çš„ç»“æ„åŒ–æ—¥å¿—"""
        if self.structured_storage:
            return self.structured_storage.cleanup_old_entries(retention_days)
        return 0
        
    def track_error(self, exception: Exception, 
                   operation: Optional[str] = None,
                   user_id: Optional[str] = None,
                   session_id: Optional[str] = None,
                   **kwargs) -> str:
        """è¿½è¸ªé”™è¯¯"""
        context = ErrorContext(
            user_id=user_id,
            session_id=session_id,
            operation=operation,
            custom_data=kwargs
        )
        return error_tracker.track_error(exception, context=context)
        
    def add_operation_log(self, operation: str, details: Optional[Dict[str, Any]] = None):
        """æ·»åŠ æ“ä½œæ—¥å¿—åˆ°é”™è¯¯è¿½è¸ªå™¨å†å²"""
        error_tracker.add_operation(operation, details)
        
    def get_error_report(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–é”™è¯¯æŠ¥å‘Š"""
        return error_tracker.get_error_report(hours)
        
    def analyze_error_patterns(self) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        from lama_cleaner.error_tracker import error_analyzer
        return error_analyzer.analyze_patterns()
        
    def export_error_report(self, filepath: str) -> bool:
        """å¯¼å‡ºé”™è¯¯æŠ¥å‘Š"""
        return error_tracker.export_errors(filepath)
        
    def log_error_report(self, hours: int = 24) -> None:
        """è®°å½•é”™è¯¯æŠ¥å‘Šåˆ°æ—¥å¿—"""
        try:
            report = self.get_error_report(hours)
            
            logger.info("=" * 50)
            logger.info("ğŸ” é”™è¯¯è¿½è¸ªæŠ¥å‘Š")
            logger.info("=" * 50)
            
            # æ‘˜è¦ä¿¡æ¯
            summary = report.get('summary', {})
            logger.info("ğŸ“Š é”™è¯¯æ‘˜è¦:")
            logger.info(f"  â€¢ æ€»é”™è¯¯æ•°: {summary.get('total_errors', 0)}")
            logger.info(f"  â€¢ å”¯ä¸€é”™è¯¯: {summary.get('unique_errors', 0)}")
            logger.info(f"  â€¢ ä¸¥é‡é”™è¯¯: {summary.get('critical_count', 0)}")
            logger.info(f"  â€¢ æœªè§£å†³æ•°: {summary.get('unresolved_count', 0)}")
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            by_category = report.get('by_category', {})
            if by_category:
                logger.info("\nğŸ“ æŒ‰ç±»åˆ«ç»Ÿè®¡:")
                for category, count in sorted(by_category.items(), key=lambda x: x[1], reverse=True):
                    logger.info(f"  â€¢ {category}: {count}")
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
            by_severity = report.get('by_severity', {})
            if by_severity:
                logger.info("\nâš ï¸ æŒ‰ä¸¥é‡ç¨‹åº¦:")
                for severity, count in by_severity.items():
                    logger.info(f"  â€¢ {severity}: {count}")
            
            # é«˜é¢‘é”™è¯¯
            top_errors = report.get('top_errors', {})
            if top_errors:
                logger.info("\nğŸ” é«˜é¢‘é”™è¯¯:")
                for error, count in list(top_errors.items())[:5]:
                    logger.info(f"  â€¢ {error}: {count}æ¬¡")
            
            # é”™è¯¯æ¨¡å¼åˆ†æ
            patterns = self.analyze_error_patterns()
            if patterns.get('patterns'):
                logger.info("\nğŸ”¬ é”™è¯¯æ¨¡å¼:")
                for pattern in patterns['patterns']:
                    logger.info(f"  â€¢ {pattern.get('description', 'æœªçŸ¥æ¨¡å¼')}")
                    if 'recommendation' in pattern:
                        logger.info(f"    å»ºè®®: {pattern['recommendation']}")
            
            logger.info("=" * 50)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆé”™è¯¯æŠ¥å‘Šå¤±è´¥: {e}")
            
    def enable_privacy_protection(self, config: Optional[PrivacyConfig] = None):
        """å¯ç”¨éšç§ä¿æŠ¤"""
        self.privacy_enabled = True
        if config:
            self.privacy_config = config
            privacy_protector.config = config
        logger.info("âœ… éšç§ä¿æŠ¤å·²å¯ç”¨")
        
    def disable_privacy_protection(self):
        """ç¦ç”¨éšç§ä¿æŠ¤"""
        self.privacy_enabled = False
        logger.warning("âš ï¸ éšç§ä¿æŠ¤å·²ç¦ç”¨")
        
    def configure_privacy(self, **kwargs):
        """é…ç½®éšç§ä¿æŠ¤é€‰é¡¹"""
        for key, value in kwargs.items():
            if hasattr(self.privacy_config, key):
                setattr(self.privacy_config, key, value)
        privacy_protector.config = self.privacy_config
        logger.info("éšç§ä¿æŠ¤é…ç½®å·²æ›´æ–°")
        
    def add_log_user(self, user_id: str, role: str = 'viewer'):
        """æ·»åŠ æ—¥å¿—è®¿é—®ç”¨æˆ·"""
        privacy_protector.access_controller.add_user(user_id, role)
        
    def check_log_access(self, user_id: str, action: str) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ—¥å¿—è®¿é—®æƒé™"""
        return privacy_protector.check_access(user_id, action)
        
    def protect_log_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿æŠ¤æ—¥å¿—æ•°æ®"""
        if self.privacy_enabled:
            return privacy_protector.protect_log_data(data)
        return data
        
    def get_privacy_report(self) -> Dict[str, Any]:
        """è·å–éšç§ä¿æŠ¤æŠ¥å‘Š"""
        return privacy_protector.export_privacy_report()
        
    def get_audit_log(self, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–è®¿é—®å®¡è®¡æ—¥å¿—"""
        return privacy_protector.get_audit_log(limit)
        
    def show_startup_banner(self, version: str = "1.0.0", mode: str = "åˆ†å¸ƒå¼å¤„ç†", 
                            host: str = "localhost", port: int = 8080) -> None:
        """æ˜¾ç¤ºå¯åŠ¨æ¨ªå¹…"""
        import platform
        from datetime import datetime
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        py_version = platform.python_version()
        os_info = platform.system()
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ASCIIè‰ºæœ¯å­—
        ascii_art = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘    ğŸ¦™ LAMA CLEANER - Image Inpainting System                â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
        
        banner = f"""{ascii_art}

ğŸš€ ç³»ç»Ÿå¯åŠ¨ä¿¡æ¯
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ğŸ“¦ ç‰ˆæœ¬:      v{version}
   ğŸ¯ è¿è¡Œæ¨¡å¼:  {mode}
   ğŸŒ ç›‘å¬åœ°å€:  http://{host}:{port}
   ğŸ“ æ—¥å¿—ç›®å½•:  ./logs/
   ğŸ–¥ï¸  ç³»ç»Ÿç¯å¢ƒ:  {os_info} | Python {py_version}
   ğŸ• å¯åŠ¨æ—¶é—´:  {timestamp}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        print(banner)
        

# å…¨å±€æ—¥å¿—ç®¡ç†å™¨å®ä¾‹
_logger_manager = LoggerManager()

def setup_logging(level: str = "INFO", enable_file_logging: bool = True) -> None:
    """è®¾ç½®å…¨å±€æ—¥å¿—é…ç½®"""
    _logger_manager.setup_logging(level, enable_file_logging)

def get_logger(name: str):
    """è·å–æ¨¡å—æ—¥å¿—å™¨"""
    return _logger_manager.get_logger(name)

def show_startup_banner(version: str = "1.0.0", mode: str = "åˆ†å¸ƒå¼å¤„ç†", 
                       host: str = "localhost", port: int = 8080) -> None:
    """æ˜¾ç¤ºå¯åŠ¨æ¨ªå¹…"""
    _logger_manager.show_startup_banner(version, mode, host, port)

def log_success(message: str, name: str = "system") -> None:
    """è®°å½•æˆåŠŸæ¶ˆæ¯"""
    logger.bind(name=name).success(message)

def log_shutdown(name: str = "system") -> None:
    """æ˜¾ç¤ºä¼˜é›…å…³é—­æ¶ˆæ¯"""
    shutdown_msg = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    ğŸ›‘ ç³»ç»Ÿæ­£åœ¨ä¼˜é›…å…³é—­...                                    â•‘
â•‘    ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ Lama Cleaner!                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(shutdown_msg)
    logger.bind(name=name).info("ç³»ç»Ÿå·²å®‰å…¨å…³é—­")