#!/usr/bin/env python3
"""
æ—¥å¿—ç®¡ç†å‘½ä»¤è¡Œå·¥å…·

æä¾›æ—¥å¿—æŸ¥è¯¢ã€ç»Ÿè®¡ã€æ¸…ç†å’Œå¯¼å‡ºåŠŸèƒ½ã€‚
"""

import argparse
import json
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

from lama_cleaner.logging_config import LoggerManager, LoggingConfig


def parse_datetime(date_str: str) -> datetime:
    """è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²"""
    formats = [
        "%Y-%m-%d",
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%d %H:%M",
        "%m-%d %H:%M",
        "%H:%M"
    ]
    
    for fmt in formats:
        try:
            if fmt == "%H:%M":
                # ä»Šå¤©çš„æ—¶é—´
                today = datetime.now().date()
                time_part = datetime.strptime(date_str, fmt).time()
                return datetime.combine(today, time_part)
            elif fmt == "%m-%d %H:%M":
                # ä»Šå¹´çš„æ—¥æœŸæ—¶é—´
                year = datetime.now().year
                dt = datetime.strptime(f"{year}-{date_str}", "%Y-%m-%d %H:%M")
                return dt
            else:
                return datetime.strptime(date_str, fmt)
        except ValueError:
            continue
            
    raise ValueError(f"æ— æ³•è§£ææ—¥æœŸæ—¶é—´: {date_str}")


def main():
    parser = argparse.ArgumentParser(description="Lama Cleaner æ—¥å¿—ç®¡ç†å·¥å…·")
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # æŸ¥è¯¢æ—¥å¿—å‘½ä»¤
    search_parser = subparsers.add_parser("search", help="æœç´¢æ—¥å¿—")
    search_parser.add_argument("query", help="æœç´¢å…³é”®è¯")
    search_parser.add_argument("--level", help="æ—¥å¿—çº§åˆ«è¿‡æ»¤")
    search_parser.add_argument("--module", help="æ¨¡å—åè¿‡æ»¤")
    search_parser.add_argument("--start", help="å¼€å§‹æ—¶é—´ (YYYY-MM-DD HH:MM:SS)")
    search_parser.add_argument("--end", help="ç»“æŸæ—¶é—´ (YYYY-MM-DD HH:MM:SS)")
    search_parser.add_argument("--limit", type=int, default=100, help="ç»“æœæ•°é‡é™åˆ¶")
    search_parser.add_argument("--format", choices=["table", "json"], default="table", help="è¾“å‡ºæ ¼å¼")
    
    # ç»Ÿè®¡ä¿¡æ¯å‘½ä»¤
    stats_parser = subparsers.add_parser("stats", help="æ˜¾ç¤ºæ—¥å¿—ç»Ÿè®¡ä¿¡æ¯")
    stats_parser.add_argument("--hours", type=int, default=24, help="ç»Ÿè®¡æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰")
    
    # é”™è¯¯æ‘˜è¦å‘½ä»¤
    error_parser = subparsers.add_parser("errors", help="æ˜¾ç¤ºé”™è¯¯æ‘˜è¦")
    error_parser.add_argument("--hours", type=int, default=24, help="ç»Ÿè®¡æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰")
    
    # æ–‡ä»¶ä¿¡æ¯å‘½ä»¤
    files_parser = subparsers.add_parser("files", help="æ˜¾ç¤ºæ—¥å¿—æ–‡ä»¶ä¿¡æ¯")
    
    # æ¸…ç†å‘½ä»¤
    cleanup_parser = subparsers.add_parser("cleanup", help="æ¸…ç†æ—§æ—¥å¿—")
    cleanup_parser.add_argument("--days", type=int, default=30, help="ä¿ç•™å¤©æ•°")
    cleanup_parser.add_argument("--dry-run", action="store_true", help="ä»…æ˜¾ç¤ºå°†è¦åˆ é™¤çš„æ–‡ä»¶")
    
    # å¯¼å‡ºå‘½ä»¤
    export_parser = subparsers.add_parser("export", help="å¯¼å‡ºæ—¥å¿—")
    export_parser.add_argument("output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    export_parser.add_argument("--start", help="å¼€å§‹æ—¶é—´")
    export_parser.add_argument("--end", help="ç»“æŸæ—¶é—´")
    export_parser.add_argument("--format", choices=["zip", "json"], default="zip", help="å¯¼å‡ºæ ¼å¼")
    
    # å¤‡ä»½å‘½ä»¤
    backup_parser = subparsers.add_parser("backup", help="å¤‡ä»½ç»“æ„åŒ–æ—¥å¿—æ•°æ®åº“")
    backup_parser.add_argument("output", help="å¤‡ä»½æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
        
    # åˆå§‹åŒ–æ—¥å¿—ç®¡ç†å™¨
    manager = LoggerManager()
    
    # å°è¯•åŠ è½½é…ç½®
    config_path = "config/logging_config.json"
    if Path(config_path).exists():
        config = manager.load_config(config_path)
        manager.setup_logging(config=config)
    else:
        manager.setup_logging(enable_file_logging=True)
        
    try:
        if args.command == "search":
            handle_search(manager, args)
        elif args.command == "stats":
            handle_stats(manager, args)
        elif args.command == "errors":
            handle_errors(manager, args)
        elif args.command == "files":
            handle_files(manager, args)
        elif args.command == "cleanup":
            handle_cleanup(manager, args)
        elif args.command == "export":
            handle_export(manager, args)
        elif args.command == "backup":
            handle_backup(manager, args)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå‘½ä»¤æ—¶å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    finally:
        manager.shutdown()


def handle_search(manager: LoggerManager, args):
    """å¤„ç†æœç´¢å‘½ä»¤"""
    filters = {}
    
    if args.level:
        filters['level'] = args.level.upper()
    if args.module:
        filters['module'] = args.module
    if args.start:
        filters['start_time'] = parse_datetime(args.start)
    if args.end:
        filters['end_time'] = parse_datetime(args.end)
    if args.limit:
        filters['limit'] = args.limit
        
    results = manager.search_logs(args.query, **filters)
    
    if not results:
        print("ğŸ” æœªæ‰¾åˆ°åŒ¹é…çš„æ—¥å¿—è®°å½•")
        return
        
    if args.format == "json":
        print(json.dumps(results, ensure_ascii=False, indent=2, default=str))
    else:
        print(f"ğŸ” æ‰¾åˆ° {len(results)} æ¡åŒ¹é…è®°å½•:\n")
        for i, log in enumerate(results, 1):
            print(f"{i:3d}. [{log['level']}] {log['timestamp']}")
            print(f"     æ¨¡å—: {log['module']}")
            print(f"     æ¶ˆæ¯: {log['message']}")
            if log.get('extra_data'):
                print(f"     é¢å¤–: {json.dumps(log['extra_data'], ensure_ascii=False)}")
            print()


def handle_stats(manager: LoggerManager, args):
    """å¤„ç†ç»Ÿè®¡å‘½ä»¤"""
    stats = manager.get_log_statistics(args.hours)
    
    if not stats:
        print("ğŸ“Š æš‚æ— ç»Ÿè®¡æ•°æ®")
        return
        
    print(f"ğŸ“Š æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯ (æœ€è¿‘ {args.hours} å°æ—¶)")
    print("=" * 50)
    print(f"æ€»è®°å½•æ•°: {stats.get('total_entries', 0)}")
    
    print("\nğŸ“ˆ çº§åˆ«åˆ†å¸ƒ:")
    for level, count in stats.get('level_distribution', {}).items():
        print(f"  {level}: {count}")
        
    print("\nğŸ—ï¸ æ¨¡å—åˆ†å¸ƒ:")
    for module, count in sorted(stats.get('module_distribution', {}).items(), 
                               key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {module}: {count}")


def handle_errors(manager: LoggerManager, args):
    """å¤„ç†é”™è¯¯æ‘˜è¦å‘½ä»¤"""
    summary = manager.get_error_summary(args.hours)
    
    if not summary:
        print("âœ… æš‚æ— é”™è¯¯è®°å½•")
        return
        
    print(f"âŒ é”™è¯¯æ‘˜è¦ ({summary.get('time_range', '')})")
    print("=" * 50)
    print(f"æ€»é”™è¯¯æ•°: {summary.get('total_errors', 0)}")
    print(f"å”¯ä¸€é”™è¯¯: {summary.get('unique_errors', 0)}")
    
    print("\nğŸ”¥ é”™è¯¯è¯¦æƒ…:")
    for message, info in summary.get('error_groups', {}).items():
        print(f"\n  é”™è¯¯: {message}")
        print(f"  æ¬¡æ•°: {info['count']}")
        print(f"  æ¨¡å—: {', '.join(info['modules'])}")
        print(f"  é¦–æ¬¡: {info['first_occurrence']}")
        print(f"  æœ€è¿‘: {info['last_occurrence']}")


def handle_files(manager: LoggerManager, args):
    """å¤„ç†æ–‡ä»¶ä¿¡æ¯å‘½ä»¤"""
    files = manager.get_log_files_info()
    
    if not files:
        print("ğŸ“ æš‚æ— æ—¥å¿—æ–‡ä»¶")
        return
        
    print("ğŸ“ æ—¥å¿—æ–‡ä»¶ä¿¡æ¯")
    print("=" * 70)
    print(f"{'æ–‡ä»¶å':<30} {'å¤§å°(MB)':<10} {'ä¿®æ”¹æ—¶é—´':<20} {'å‹ç¼©':<6}")
    print("-" * 70)
    
    total_size = 0
    for file_info in files:
        compressed = "âœ“" if file_info['compressed'] else "âœ—"
        print(f"{file_info['name']:<30} {file_info['size_mb']:<10.2f} "
              f"{file_info['modified'].strftime('%Y-%m-%d %H:%M'):<20} {compressed:<6}")
        total_size += file_info['size_mb']
        
    print("-" * 70)
    print(f"æ€»è®¡: {len(files)} ä¸ªæ–‡ä»¶, {total_size:.2f} MB")


def handle_cleanup(manager: LoggerManager, args):
    """å¤„ç†æ¸…ç†å‘½ä»¤"""
    if args.dry_run:
        print(f"ğŸ§¹ æ¨¡æ‹Ÿæ¸…ç† (ä¿ç•™ {args.days} å¤©)")
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡æ‹Ÿæ¸…ç†çš„é€»è¾‘
        print("æ³¨æ„: è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œï¼Œå®é™…æ–‡ä»¶ä¸ä¼šè¢«åˆ é™¤")
    else:
        print(f"ğŸ§¹ å¼€å§‹æ¸…ç†æ—¥å¿— (ä¿ç•™ {args.days} å¤©)")
        
        # æ¸…ç†æ–‡ä»¶æ—¥å¿—
        result = manager.manual_cleanup()
        print(f"æ–‡ä»¶æ¸…ç†å®Œæˆ: åˆ é™¤äº† {result.get('deleted', 0)} ä¸ªæ–‡ä»¶")
        
        # æ¸…ç†ç»“æ„åŒ–æ—¥å¿—
        deleted_entries = manager.cleanup_structured_logs(args.days)
        print(f"æ•°æ®åº“æ¸…ç†å®Œæˆ: åˆ é™¤äº† {deleted_entries} æ¡è®°å½•")


def handle_export(manager: LoggerManager, args):
    """å¤„ç†å¯¼å‡ºå‘½ä»¤"""
    start_time = parse_datetime(args.start) if args.start else None
    end_time = parse_datetime(args.end) if args.end else None
    
    print(f"ğŸ“¦ å¼€å§‹å¯¼å‡ºæ—¥å¿—åˆ°: {args.output}")
    
    if args.format == "json":
        success = manager.export_structured_logs(args.output, start_time, end_time)
    else:
        date_range = (start_time, end_time) if start_time and end_time else None
        success = manager.export_logs(args.output, date_range)
        
    if success:
        print("âœ… å¯¼å‡ºå®Œæˆ")
    else:
        print("âŒ å¯¼å‡ºå¤±è´¥")
        sys.exit(1)


def handle_backup(manager: LoggerManager, args):
    """å¤„ç†å¤‡ä»½å‘½ä»¤"""
    print(f"ğŸ’¾ å¼€å§‹å¤‡ä»½ç»“æ„åŒ–æ—¥å¿—æ•°æ®åº“åˆ°: {args.output}")
    
    success = manager.backup_structured_logs(args.output)
    
    if success:
        print("âœ… å¤‡ä»½å®Œæˆ")
    else:
        print("âŒ å¤‡ä»½å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()